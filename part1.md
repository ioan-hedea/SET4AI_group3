# Part 1

# Validity Problem — The Target Label [“checked”] does NOT actually represent welfare fraud risk
The dataset’s binary label checked was generated by applying a threshold on a synthetic risk score (‘ja’) that itself was reconstructed from the original Rotterdam algorithm.
-	The original risk labels were not ground truth about fraud, but administrative decisions based on a controversial algorithm.
-	Investigations by Lighthouse Reports showed that the original algorithm was biased and discriminatory, meaning the label is heavily polluted with past institutional bias.
-	Therefore, the label does not measure real fraud, but who the system thought should be checked, which is a very different concept.
A prediction target must meaningfully represent the phenomenon of interest. Here, the “risk” label is a self-fulfilling administrative artifact, not a measure of actual fraud risk. 
- Any model trained on this inherits and reinforces historical discrimination.




# Many features are proxies for protected attributes (ethnicity, migration background, socioeconomic status)

The dataset includes features such as:
-	adres_recentste_plaats_rotterdam vs adres_recentste_plaats_other
-	Neighborhood indicators (...buurt_groot_ijsselmonde, ...wijk_delfshaven, etc.)
-	Language proficiency indicators (persoonlijke_eigenschappen_nl_lezen3, ...spreken1, etc.)
-	Indicators tied to household structure
-	Indicators tied to type of contact with the municipality

These strongly correlate with protected attributes like:
-	Ethnicity / migration background
-	Socioeconomic class
-	Income bracket
-	Education level

Even when a feature is “not explicitly ethnicity”, geographic and language features act as high-fidelity proxies.
The model can easily learn discriminatory patterns, even if protected attributes are not directly included. 
This undermines fairness and violates the assignment’s goal of detecting such hidden biases.


# Extremely high dimensionality with many sparse, binary, and noisy variables

The dataset contains hundreds of columns(316), many of which are:
-	Binary indicators for rare administrative events
-	Highly correlated or duplicated fields
-	Encoded as “1 event” vs “0 event” without context
-	Automatically generated categories from municipal logs
-	Sparse activity logs (many zeros)

This causes several issues:
-	Models may overfit on random noise (e.g., one rare administrative code becomes predictive).
-	Some fields are mechanically derived from internal systems rather than representing true citizen behavior.
-	Many variables represent bureaucratic processes, not personal characteristics (e.g., number of contact entries with UWV, reporting codes, etc.).

Features do not all represent meaningful behaviors of citizens — instead, they reflect internal workflow patterns of municipal employees. 
This distorts what the model learns, introducing bias related to caseworker behavior.

